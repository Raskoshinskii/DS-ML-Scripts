{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters Tuning\n",
    "The are several main techniques thah can be used for hyperparameters tunning\n",
    "\n",
    "### GridSearchCV\n",
    "Performs an exhaustive search over the specified range of hyperparameters (grid). For this method you need to specify every single value for each parameter (which can be tricky, especially for the continuous parameters) that you want your model to try.\n",
    "\n",
    "**The main disadvantages:**\n",
    "- If searching space is large, it takes forever\n",
    "- Discrete set of parameters (if optimum value is 150 but the range is `[100, 200]`, the optimum won't be found)\n",
    "\n",
    "### Randomized Search CV\n",
    "Doesn’t set up a grid of hyperparameter values. Instead, we have to specify a distribution for each hyperparameter we want to tune. Randomized Search CV then sample values from these distributions and selects their random combinations. \n",
    "\n",
    "But still the optimum set of hyperparameters can be missed due to the randomness of the algorithm.\n",
    "\n",
    "### Bayesian Methods\n",
    "More advanced approaches are using the history of past trials to select hyperparameters for each trial in an informed manner. This often results in the faster hyperparameter tuning process and more accurate resulting models. There are several modules that allow implementing this algorithm:\n",
    "- Hyperopt\n",
    "- Optuna \n",
    "\n",
    "### Hyperopt Implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from hyperopt import tpe, hp, fmin, space_eval, Trials\n",
    "from hyperopt.pyll.stochastic import sample as ho_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelHyperparameters:\n",
    "    \n",
    "    def __init__(self, model, x, y, params_space, n_trials, cv_metric, cv_type, fit_params=None, opt_algo=tpe.suggest, seed=23):\n",
    "        self.model = model\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.params_space = params_space\n",
    "        self.n_trials = n_trials\n",
    "        self.cv_metric = cv_metric\n",
    "        self.cv_type = cv_type\n",
    "        self.fit_params = fit_params\n",
    "        self.opt_algo = opt_algo\n",
    "        self.seed = seed\n",
    "        self.trials = Trials()\n",
    "        \n",
    "    def _objective(self, params):\n",
    "        \n",
    "        self.model.set_params(**params)\n",
    "        \n",
    "        cv_score = cross_val_score(self.model,\n",
    "                                   self.x,\n",
    "                                   self.y,\n",
    "                                   scoring=self.cv_metric,\n",
    "                                   cv=self.cv_type,\n",
    "                                   error_score='reaise',\n",
    "                                   fit_params=self.fit_params,\n",
    "                                   n_jobs=-1)\n",
    "        \n",
    "        return -cv_score.mean()\n",
    "        \n",
    "    def optimize(self):\n",
    "        return fmin(fn=self._objective,\n",
    "                    space=self.params_space,\n",
    "                    algo=self.opt_algo,\n",
    "                    max_evals=self.n_trials,\n",
    "                    trials=self.trials,\n",
    "                    rstate=np.random.RandomState(self.seed))\n",
    "    \n",
    "# model_hyperparameters = ModelHyperparameters(model=model_name, x=x_train, y=y_train,\n",
    "#                                              params_space=params_space, n_trials=50, cv_metric='roc_auc',\n",
    "#                                              cv_type=StratifiedKFold(shuffle=True, random_state=SEED))\n",
    "\n",
    "# best_params = model_hyperparameters.optimize()\n",
    "\n",
    "# For better found parameters retrieval use space_eval\n",
    "# space_eval(params_space, best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GradientBoostingClassifier\n",
    "params_space = {\n",
    "    'model__learning_rate': hp.loguniform('learning_rate', np.log(0.001), np.log(0.5)),\n",
    "    'model__n_estimators': ho_scope.int(hp.quniform('n_estimators', 50, 500, 1)),\n",
    "    'model__criterion': hp.choice('criterion', ['friedman_mse', 'mse']), # don't use mae\n",
    "    'model__min_samples_split': hp.loguniform('min_samples_split', np.log(0.1), np.log(1)),\n",
    "    'model__max_depth':  ho_scope.int(hp.quniform('max_depth', 1, 8, 1)),\n",
    "    'model__max_features': ho_scope.int(hp.quniform('max_features', 1, x_train.shape[1], 1)),\n",
    "    'model__random_state': SEED\n",
    "} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optuna "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna \n",
    "from optuna import samplers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelHyperparameters:\n",
    "    \n",
    "    def __init__(self, model, x, y, n_trials, cv_metric, cv_type, opt_algo='tpe', direction='maximize', seed=23):\n",
    "        self.model = model\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.n_trials = n_trials\n",
    "        self.cv_metric = cv_metric\n",
    "        self.cv_type = cv_type\n",
    "        self.direction = direction\n",
    "        self.seed = seed\n",
    "        self.study = None\n",
    "        \n",
    "        if opt_algo == 'tpe':\n",
    "            self.opt_algo = samplers.TPESampler(self.seed)\n",
    "        \n",
    "        \n",
    "    def _objective(self, trial):\n",
    "        params_space = {} # define model params here\n",
    "        \n",
    "        self.model.set_params(**params_space) # для optuna работает только так \n",
    "        \n",
    "        cv_score = cross_val_score(self.model, self.x, self.y,\n",
    "                                   scoring=self.cv_metric, cv=self.cv_type, n_jobs=-1)\n",
    "        return cv_score.mean()\n",
    "    \n",
    "    def optimize(self):\n",
    "        self.study = optuna.create_study(sampler=self.opt_algo, direction=self.direction)\n",
    "        self.study.optimize(self._objective, n_trials=self.n_trials)\n",
    "        \n",
    "        \n",
    "# model_hyperparameters = ModelHyperparameters(model=model_name, x=x_train, y=y_train,\n",
    "#                                              n_trials=50, cv_metric='roc_auc',\n",
    "#                                              cv_type=StratifiedKFold(shuffle=True, random_state=SEED))\n",
    "\n",
    "# model_hyperparameters.optimize()\n",
    "# model_hyperparameters.study.best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Некторые приближения гиперпараметров для моделей \n",
    "\n",
    "### GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optuna\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "params = {\n",
    "    'learning_rate': trial.suggest_uniform('learning_rate', 0.001, 0.5),\n",
    "    'n_estimators': trial.suggest_int('n_estimators', 40, 1000),\n",
    "    'criterion': trial.suggest_categorical('criterion', ['friedman_mse', 'mse']),\n",
    "    'min_samples_split': trial.suggest_uniform('learning_rate', 0.1, 1),\n",
    "    'min_samples_leaf': trial.suggest_uniform('learning_rate', 0.1, 1),\n",
    "    'max_depth': trial.suggest_int('max_depth', 1, 10),\n",
    "    'max_features': trial.suggest_int('max_features', 1, x_train.shape[1])\n",
    "}    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optuna\n",
    "from xgboost import XGBClassifier\n",
    "xgboost_model = XGBClassifier(**params)\n",
    "\n",
    "params = {\n",
    "    'n_estimators': trial.suggest_int('n_estimators', 40, 600),\n",
    "    'max_depth': trial.suggest_int('max_depth', 2, 20),\n",
    "    'min_child_weight': trial.suggest_int('min_child_weight', 2, 20),\n",
    "    'learning_rate': trial.suggest_uniform('learning_rate', 0.001, 0.5),\n",
    "    'base_score': trial.suggest_uniform('base_score', 0.01, 1),\n",
    "    'subsample': trial.suggest_uniform('subsample', 0.50, 1),\n",
    "    'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.50, 1),\n",
    "    'colsample_bynode': trial.suggest_uniform('colsample_bytree', 0.50, 1),\n",
    "    'colsample_bylevel': trial.suggest_uniform('colsample_bytree', 0.50, 1),\n",
    "    'gamma': trial.suggest_int('gamma', 0, 10),\n",
    "    'tree_method': 'gpu_hist',  \n",
    "    'objective': 'binary:logistic'\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
