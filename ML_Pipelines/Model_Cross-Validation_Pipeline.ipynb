{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Pipeline\n",
    "Pipeline that can process **only one type of features** numeric/categorical (pipe must be redifined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_models_cv_pipeline(models, cv_type, x_train, y_train, metrics, is_aggregated=True):\n",
    "    \"\"\"\n",
    "    models: list \n",
    "        List of model candidates\n",
    "        \n",
    "    cv_type: cross-validation type\n",
    "    \n",
    "    metrics: list \n",
    "        Metric to calculate\n",
    "        \n",
    "    is_aggregated: bool\n",
    "        Wether to return aggregated results or on each fold\n",
    "    \"\"\"\n",
    "    res_df = pd.DataFrame()\n",
    "    \n",
    "    for model in log_progress(models):\n",
    "        model_pipeline = Pipeline([\n",
    "            ('data_imputing', KNNImputer()),\n",
    "            ('data_scaling', StandardScaler()),\n",
    "            ('model', model)\n",
    "        ])\n",
    "    \n",
    "        cv_results = cross_validate(model_pipeline, X=x_train, y=y_train, cv=cv_type, scoring=metrics)\n",
    "        cv_results['Model'] = str(model).split('(')[0] # extract model name\n",
    "        res_df = res_df.append(pd.DataFrame(cv_results))\n",
    "            \n",
    "    # For making the name of the model the first\n",
    "    new_columns_order = list(res_df.columns[2:-1])\n",
    "    new_columns_order.insert(0, 'Model')\n",
    "    \n",
    "    # Returning results either on each fold or aggregated \n",
    "    if is_aggregated:\n",
    "        return res_df[new_columns_order].groupby(by='Model').mean()\n",
    "    else:\n",
    "        return res_df[new_columns_order]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline All Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Переопределим функцию make_pipe, добавив блок final_imputing ( необходимо добавить, т.к. появляется inf, которую заменяем на 0)\n",
    "def make_pipe(cat_bin_columns,\n",
    "              num_columns,\n",
    "              model=None,\n",
    "              cat_bin_imputer=SimpleImputer(strategy='constant', fill_value='unknown'),\n",
    "              cat_bin_encoder=OneHotEncoder(sparse=True, handle_unknown='ignore'),\n",
    "              num_imputer=SimpleImputer(strategy='mean'),\n",
    "              num_scaler=StandardScaler()):\n",
    "    \n",
    "    # Categorical/Binary Features Processing\n",
    "    cat_bin_pipeline = Pipeline([\n",
    "        ('cat_bin_imputing', cat_bin_imputer),\n",
    "        ('cat_bin_encoding', cat_bin_encoder),\n",
    "        ('final_imputing', SimpleImputer(strategy='constant', fill_value=0)) # Полученные inf заменяем на 0\n",
    "    ])\n",
    "    \n",
    "    # Numerical Features Processing\n",
    "    num_pipeline = Pipeline([\n",
    "        ('num_imputing', num_imputer),\n",
    "        ('num_scaling', num_scaler)\n",
    "    ])\n",
    "    \n",
    "    # Main Transformations\n",
    "    transformations = [\n",
    "        ('cat_bin_transformations', cat_bin_pipeline, cat_bin_columns),\n",
    "        ('num_transformations', num_pipeline, num_columns)\n",
    "    ]\n",
    "    \n",
    "    feature_transformations = ColumnTransformer(transformers=transformations, n_jobs=-1)\n",
    "    \n",
    "    main_pipeline = Pipeline([\n",
    "        ('feature_transformations', feature_transformations)\n",
    "    ])\n",
    "    \n",
    "    if model is not None:\n",
    "        main_pipeline.steps.insert(1, ('model', model))\n",
    "        return main_pipeline\n",
    "    else:\n",
    "        return main_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline + ColumnTransformer (All Features)\n",
    "Cross-validation for different models, imputers, encoders and scalers...processes all features independently and returns cross-validation score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import StratifiedKFold, cross_validate\n",
    "\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "\n",
    "from tqdm.notebook import tqdm as log_progress\n",
    "\n",
    "def show_model_cvs_pipeline_all_features(models, x_train, y_train, \n",
    "                                         cv_type, metrics,\n",
    "                                         num_columns, cat_columns, bin_columns, \n",
    "                                         bin_imputer=SimpleImputer(strategy='most_frequent'), bin_encoder=OrdinalEncoder(),\n",
    "                                         cat_imputer=SimpleImputer(strategy='most_frequent'), cat_encoder=OneHotEncoder(sparse=True, handle_unknown='ignore'),\n",
    "                                         num_imputer=KNNImputer(), scaler=StandardScaler(),\n",
    "                                         is_aggregated=True):\n",
    "    \"\"\"\n",
    "    models - list of tested models\n",
    "    cv_type - type of cross-validation (e.g. StratifiedKFold(shuffle=True, random_state=SEED))\n",
    "    metrics - list of metrics to calculate (e.g. ['precision', 'recall', 'f1', 'roc_auc'])\n",
    "    is_aggregated - return the info either for each fold or average estimation\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Pipeline Definition\n",
    "\n",
    "    # Binary Features Processing \n",
    "    binary_pipeline = Pipeline([\n",
    "        ('binary_imputing', bin_imputer), # Any Imputer Here\n",
    "        ('binary_encoding', bin_encoder) # Any Encoder Here\n",
    "    ])\n",
    "\n",
    "    # Categorical Features Processing \n",
    "    cat_pipeline = Pipeline([\n",
    "        ('cat_imputing', cat_imputer),\n",
    "        ('cat_encoding', cat_encoder)\n",
    "    ])\n",
    "\n",
    "    # Numerical Features Processing\n",
    "    num_pipeline = Pipeline([\n",
    "        ('data_imputing', num_imputer),\n",
    "        ('data_scaling', scaler)\n",
    "    ])\n",
    "\n",
    "    transformations = [\n",
    "        ('num_transformations', num_pipeline, num_columns),\n",
    "        ('bin_transformations', binary_pipeline, bin_columns),\n",
    "        ('cat_transformations', cat_pipeline, cat_columns)\n",
    "    ]\n",
    "\n",
    "    feature_transformations = ColumnTransformer(transformers=transformations)\n",
    "    \n",
    "    res_df = pd.DataFrame() # CV results will be stored here\n",
    "    \n",
    "    # CV part for provided models\n",
    "    for model in log_progress(models):\n",
    "        model_pipeline = Pipeline([\n",
    "            ('feature_transformations', feature_transformations),\n",
    "            ('model', model)\n",
    "        ])\n",
    "        \n",
    "        cv_results = cross_validate(model_pipeline, X=x_train, y=y_train, cv=cv_type,\n",
    "                                    scoring=metrics, error_score='raise', n_jobs=-1)\n",
    "        \n",
    "        cv_results['Model'] = str(model).split('(')[0] # extract the name of the current model\n",
    "        res_df = res_df.append(pd.DataFrame(cv_results))\n",
    "\n",
    "    # Make the first column store the name of the model + drop unnecessary columns  \n",
    "    new_columns_order = list(res_df.columns[2:-1])\n",
    "    new_columns_order.insert(0, 'Model')\n",
    "    \n",
    "    if is_aggregated:\n",
    "        return res_df[new_columns_order].groupby(by='Model').mean()\n",
    "    else:\n",
    "        return res_df[new_columns_order]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Predictions Using Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for saving the test predictions\n",
    "def to_submission(data, f_name):\n",
    "    result = pd.DataFrame({'Id':range(data.shape[0]),\n",
    "                           'result':data[:,1]})\n",
    "    result.to_csv(f_name, index=False)\n",
    "\n",
    "# Make predictions and saves them if needed\n",
    "def make_test_preds_pipeline(models, x_train, y_train, x_test, f_names=None, save_results=True):\n",
    "    \"\"\"\n",
    "    models - list of models\n",
    "    f_names - list of file names    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    predictions = []\n",
    "    \n",
    "    for model in log_progress(models):\n",
    "        model_pipeline = Pipeline([\n",
    "            ('feature_transformations', feature_transformations),\n",
    "            ('model', model)\n",
    "        ])\n",
    "        model_pipeline.fit(x_train, y_train)\n",
    "        predictions.append(model_pipeline.predict_proba(x_test))\n",
    "        \n",
    "    if save_results:\n",
    "        for indx, pred in enumerate(predictions):\n",
    "            to_submission(pred, f_name=f_names[indx])\n",
    "    else:\n",
    "        return predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
