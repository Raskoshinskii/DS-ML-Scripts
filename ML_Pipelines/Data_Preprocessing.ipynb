{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NaN Ratio Hist\n",
    "Plots a histogram of NaN values for features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_nan_ration(df, figsize=(10,5), title='Numerical Features'):\n",
    "    \"\"\"\n",
    "    Plots a histogram with NaN ration and its features \n",
    "    It only takes into account the fact that the target features is located in the end of a DataFrame!\n",
    "    \n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    plt.hist(df.iloc[:,:-1].isna().sum()/df.shape[0])\n",
    "    plt.xlabel('NaN Fraction')\n",
    "    plt.ylabel('Number of Features')\n",
    "    plt.title(title)\n",
    "    plt.grid();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NaN Columns Dropping\n",
    "Drops columns that consists of only NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_nan_columns(df):\n",
    "    \"\"\"\n",
    "    Drops columns that have only NaN values\n",
    "    \n",
    "    \"\"\"\n",
    "    feat_to_drop = [feature for feature in df.columns if df[feature].isna().all()]\n",
    "\n",
    "    print(f'Dropped {len(feat_to_drop)} NaN Columns')\n",
    "    df.drop(columns=feat_to_drop, axis=1, inplace=True)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unique Categories\n",
    "Returns info about a number of unique categories per feature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_unique_cat(df): \n",
    "    \n",
    "    features = []\n",
    "    values = []\n",
    "    \n",
    "    for feature in df.columns:\n",
    "        features.append(feature)\n",
    "        values.append(df[feature].unique().shape[0])\n",
    "        \n",
    "    return pd.DataFrame({'Unique Categories': values}, index=features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers Detection\n",
    "Excludes observations that are outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(df_in, col = ''):\n",
    "    \"\"\"\n",
    "    Calculates quantiles for outliers + interquartile range. Everything above and below the borders will not be included\n",
    "    in final dataframe.\n",
    "    \n",
    "    \"\"\"\n",
    "    Q1 = df_in[col].quantile(0.25)\n",
    "    Q3= df_in[col].quantile(0.75)\n",
    "    IRQ = Q3 - Q1\n",
    "    upper_border = Q3 + IRQ*1.5\n",
    "    lower_border = Q1 - IRQ*1.5\n",
    "    df_out = df_in[ (df_in[col]>=lower_border) & (df_in[col]<=upper_border) ]\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multicollinearity Problems\n",
    "Several emplementations of dropping highly correlated features:\n",
    "- Based on correlation (usually faster)\n",
    "- Based on VIF (always slower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on Pearson Correlation\n",
    "def drop_cor_features(df, thresh = 0.8):\n",
    "            \n",
    "    corr_matrix = df.corr()\n",
    "    corr_features = set()\n",
    "\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i):\n",
    "            if abs(corr_matrix.iloc[i,j]) >= thresh:\n",
    "                col_name = corr_matrix.columns[i]\n",
    "                corr_features.add(col_name)\n",
    "        \n",
    "    print('Original Number of Features: ', df.shape[1])\n",
    "    print('Number of Higly Correlated Features: ', len(corr_features))\n",
    "    df.drop(columns=corr_features, inplace=True)\n",
    "    print('New Number of Features: ', df.shape[1])\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Based on VIF \n",
    "def caclculate_vif(df, target=None, thresh=5):\n",
    "    # df preparation\n",
    "    if target is not None:\n",
    "        df = df.drop(columns = target.columns)\n",
    "        \n",
    "    col_to_drop = df.select_dtypes(['object','datetime64[ns]']).columns\n",
    "    df = df.drop(columns = col_to_drop)\n",
    "    df = df.dropna(axis=0)\n",
    "    from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "    \n",
    "    features = df.columns\n",
    "    feature_idx = np.arange(len(features))\n",
    "    dropping = True\n",
    "    original_shape = df.shape[1]\n",
    "    print('Original Number of Features: ', original_shape)\n",
    "    # VIF Calculation\n",
    "    while dropping:\n",
    "        dropping = False\n",
    "        current_df = df[features[feature_idx]].values\n",
    "        vif_values = [ variance_inflation_factor(current_df, idx) for idx in np.arange(current_df.shape[1]) ]\n",
    "        \n",
    "        max_vif = max(vif_values)\n",
    "        max_vif_loc = vif_values.index(max_vif)\n",
    "        \n",
    "        if max_vif > thresh:\n",
    "            feature_idx = np.delete(feature_idx, max_vif_loc)\n",
    "            dropping = True\n",
    "            \n",
    "    new_shape = df[features[feature_idx]].shape[1]\n",
    "    print('Number of Features Having Large VIF: ', original_shape - new_shape)\n",
    "    print('New Number of Features:', new_shape)\n",
    "    \n",
    "    return df[features[feature_idx]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Normalization and Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data(x_in, scaling_type='N',col_to_scale=[]):\n",
    "\n",
    "    if scaling_type == 'N':\n",
    "        from sklearn.preprocessing import MinMaxScaler\n",
    "        scaler = MinMaxScaler()\n",
    "        x_in[col_to_scale] = scaler.fit_transform(x_in[col_to_scale])\n",
    "        x_norm = x_in\n",
    "        \n",
    "        return x_norm\n",
    "    \n",
    "    if scaling_type == 'S':\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        scaler = StandardScaler()\n",
    "        x_in[col_to_scale] = scaler.fit_transform(x_in[col_to_scale])\n",
    "        x_std = x_in\n",
    "        \n",
    "        return x_std"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
